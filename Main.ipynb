{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aed8bf63",
   "metadata": {},
   "source": [
    "# Main program used to remove artifacts from magnetic resonance images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc3a406",
   "metadata": {},
   "source": [
    "#### First we have to process the DICOM files in order to save it in .jgp format, add the required artifact and then save the transformed image as .jpg.\n",
    "\n",
    "Preprocessing function has seven arguments:\n",
    "\n",
    "    - path (string): the original DICOM folder.\n",
    "    \n",
    "    - BodyPart (string): it can be either 'brain' or 'knee'.\n",
    "    \n",
    "    - TypeArtifact (list of strings): the elements can be 'Motion', 'Ghosting', 'BiasField', 'Blur', 'Noise' and 'Spike'.\n",
    "    \n",
    "    - showImage (boolean): True if you want images to be shown, False if not.\n",
    "    \n",
    "    - saveImage (boolean): True if you want images to be saved, False if not.\n",
    "    \n",
    "    - multiprocess (boolean): True if you want to use multiprocessing, False if not. Default value: False.\n",
    "    \n",
    "    - num_process (int): number of processors you want to use. Default value: multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825fcf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing\n",
    "preprocessing.Preprocessing('fastMRI_brain_DICOM','brain',['Motion','Blur','BiasField','Spike','Noise','Ghosting'],False,True,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a40af9",
   "metadata": {},
   "source": [
    "#### Now, we will divide the data into training, validation and testing sets. \n",
    "\n",
    "It's important to mention that this division will be made at the level of studies (patients), rather than at the level of images, so that we ensure that the different sets don't include images of the same brains. The classification of the studies in Training, Validation and Test will be the same for all the artifacts.\n",
    "\n",
    "splitTrainValTest has just two arguments:\n",
    "\n",
    "    - path (string): the original DICOM folder.\n",
    "    \n",
    "    - TypeArtifact (list of strings): the elements can be 'Motion', 'Ghosting', 'BiasField', 'Blur', 'Noise' and 'Spike'.\n",
    "    \n",
    "Note that the artifacts that we can include in the list are no more than those that we have included in the previous function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79eb9e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import train_val_test\n",
    "train_val_test.splitTrainValTest('fastMRI_brain_DICOM',['Motion','Blur','BiasField','Spike','Noise','Ghosting'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e058c6c",
   "metadata": {},
   "source": [
    "#### We will now train our model.  \n",
    "\n",
    "It will return the model already trained. Note that the architecture of the net must be changed in training_process.py file.\n",
    "\n",
    "train_model has the following arguments:\n",
    "\n",
    "    - type_artifact (string): it can be 'Motion', 'Ghosting', 'BiasField', 'Blur', 'Noise' and 'Spike'.\n",
    "    \n",
    "    - batch_size (int): the batch size that we want for our model.\n",
    "    \n",
    "    - learning_rate (float): the learning rate hyperparameter of our model.\n",
    "    \n",
    "    - num_epochs (int): the number of epoch that the training process will have.\n",
    "    \n",
    "    - filters_code (string): it represents :\n",
    "         \n",
    "         - the kernel size in case there are 64 filters for the first layer and 32 for the second. It can be '515', '915', '935' and '955'.\n",
    "         \n",
    "         - the number of layers of the model. It can be '95', '915', '9115' and '91115'.\n",
    "         \n",
    "         - the number of filters in each layer, with kernel size 9-1-5. It can be '16_8', '32_16', '64_32' and '128_64'.\n",
    "      \n",
    "Note that the model architecture used should be changed inside the file, and filters_code should have the name corresponding to it.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc139b98-32ba-42fc-bed6-ed6b349f7925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import training_process\n",
    "model = training_process.train_model('Blur', 16, 0.001, 40, '915')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cbde01",
   "metadata": {},
   "source": [
    "#### We can now test any model we want.  \n",
    "\n",
    "It will return a vector with the loss and the accuracies of the model as (loss, SSIM, PSNR, MS-SSIM). \n",
    "\n",
    "test_model has the following arguments:\n",
    "\n",
    "    - type_artifact (string): it can be 'Motion', 'Ghosting', 'BiasField', 'Blur', 'Noise' and 'Spike'.\n",
    "    \n",
    "    - model (object of class ArtifactCNN): the model already loaded.\n",
    "    \n",
    "    - filters_code (string): the same as it was explained before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983463e3-ec5b-4f3b-a98b-eaba2899d3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import training_process\n",
    "import test_process\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "filters_code = '915'\n",
    "artifact = 'Blur'\n",
    "\n",
    "global ArtifactCNN\n",
    "class ArtifactCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ArtifactCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=9,padding=4)\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=1,padding=0)\n",
    "        self.conv3 = nn.Conv2d(32, 1, kernel_size=5,padding=2)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "    \n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu' \n",
    "model = ArtifactCNN().to(device)\n",
    "model = torch.load(f'trained_models/modelo20k_{artifact}_{filters_code}.pth')\n",
    "model.eval()\n",
    "\n",
    "test_loss,test_ssim,test_psnr,test_ms_ssim = test_process.test_model(artifact,model, filters_code)\n",
    "print(test_loss,test_ssim,test_psnr,test_ms_ssim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7cb4ca-0456-40d5-ad1d-df0b885a55a2",
   "metadata": {},
   "source": [
    "#### We can plot the training data.\n",
    "\n",
    "It will show and save the graph with the required data if we ask it to.\n",
    "\n",
    "plots has the following artifacts:\n",
    "\n",
    "    - list_artifacts (list of strings): the elements can be 'Motion', 'Ghosting', 'BiasField', 'Blur', 'Noise' and 'Spike'. They can be repeated if different models are considered\n",
    "    \n",
    "    - list_models (list of arrays): the elements can be '915', '935', '515'..., i.e. the name of the models that have been trained\n",
    "    \n",
    "    - metric (string): it can be 'MSE', 'SSIM' and 'PSNR'\n",
    "    \n",
    "    - show_fig (boolean): it indicates whether you want to show the image or not. The default value is True\n",
    "    \n",
    "    - save_fig (boolean): it indicates whether you want to save the image or not. The default value is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec061ffe-a509-406a-9660-065997363676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import training_plots\n",
    "training_plots.plots(['Blur','Spike','Motion','Ghosting','BiasField','Noise'],['915','915','915','915','915','915'],'PSNR',show_fig=True,save_fig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89782d9a-4b0b-4d16-9dc8-111562c514aa",
   "metadata": {},
   "source": [
    "#### We can also plot the filters and feature maps of a certain model.\n",
    "It will show the filters and the feature maps of a given model. \n",
    "\n",
    "function feature_maps has five parameters:\n",
    "\n",
    "    - image_path (string): it is the path of the image we want\n",
    "    \n",
    "    - model (object of the class ArtifactCNN): the model we want to get the feature maps of\n",
    "    \n",
    "    - artifact (string): the artifact of the corresponding model\n",
    "    \n",
    "    - filters_code (string): it indicates the name of the corresponding model\n",
    "    \n",
    "    - save_fig (boolean): it indicates whether you want to save the image or not. The default value is True\n",
    "    \n",
    "while 'filters' has the four last described ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1070ce5-ae5b-495a-a6bc-68386b278468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import training_process\n",
    "import post_training\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "filters_code = '915'\n",
    "artifact = 'Noise'\n",
    "\n",
    "global ArtifactCNN\n",
    "class ArtifactCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ArtifactCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=9,padding=4)\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=1,padding=0)\n",
    "        self.conv3 = nn.Conv2d(32, 1, kernel_size=5,padding=2)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "    \n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu' \n",
    "model = ArtifactCNN().to(device)\n",
    "model = torch.load(f'trained_models/modelo20k_{artifact}_{filters_code}.pth')\n",
    "model.eval()\n",
    "\n",
    "post_training.filters(model, artifact, filters_code, save_fig = False)\n",
    "post_training.feature_maps('fastMRI_brain_DICOM/Validation/106643169658/1015.jpg', model, artifact, filters_code, save_fig = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
